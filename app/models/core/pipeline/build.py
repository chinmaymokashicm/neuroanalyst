from .base import BIDSPipelineTree, DatasetDescription, GeneratedBy
from ..process.execute import ProcessExecApptainer
from ..process.test import check_process_exec_status
from ....utils.db import Connection, insert_to_db, update_db_record, find_one_from_db, check_connection
from ....utils.constants import *
from ....utils.generate import generate_id
from ....utils.dataset import get_sidecar_info_from_dataset

from typing import Optional, Any
from pathlib import Path, PosixPath
from enum import Enum
import asyncio
import time, json
from asyncio import sleep

from pydantic import BaseModel, Field, DirectoryPath
from bids import BIDSLayout
from bids.layout.models import BIDSJSONFile

class PipelineStatus(str, Enum):
    CREATED = "created"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

class PipelineStep(BaseModel):
    id: str = Field(title="The ID of the pipeline step.", default_factory=lambda: generate_id("PS", 6, "-"))
    name: str = Field(title="The name of the pipeline step.")
    process_execs: list[ProcessExecApptainer] = Field(title="List of process execution plans.")
    status: PipelineStatus = Field(title="The status of the pipeline step.", default=PipelineStatus.CREATED)
    metrics: Optional[list[dict]] = Field(title="Metrics generated by the pipeline step. Loaded from the dataset after execution.", default=None)
    
    @classmethod
    def from_user(cls, name, process_exec_ids: list[str]) -> "PipelineStep":
        process_execs: list[ProcessExecApptainer] = [ProcessExecApptainer.from_db(process_exec_id) for process_exec_id in process_exec_ids]
        return cls(
            name=name,
            process_execs=process_execs
        )
    
    def execute(self, pipeline_id: str, save_to_db: bool = True):
        # ! Run this in Celery
        self.status = PipelineStatus.RUNNING
        
        # Trigger run all processes in parallel
        for process_exec in self.process_execs:
            try:
                process_id, process_exec_id = process_exec.process_image.id, process_exec.id
                additional_envs: dict = {"PROCESS_ID": process_id, "PROCESS_EXEC_ID": process_exec_id, "PIPELINE_ID": pipeline_id}
                process_exec.execute(save_to_db=save_to_db, additional_envs=additional_envs)
            except Exception as e:
                self.status = PipelineStatus.FAILED
                raise Exception(f"Failed to execute process execution {process_exec.id}. Error: {e}")
        
        # Monitor for all processes to complete
        # while self.status == PipelineStatus.RUNNING:
        #     print(f"Checking status of process execution {process_exec.id} for {self.id}")
        #     time.sleep(5)
        #     process_exec_statuses = [check_process_exec_status(process_exec.id)["status"] for process_exec in self.process_execs]
        #     if "FAILED" in process_exec_statuses:
        #         print(f"Process execution FAILED for {self.id}")
        #         self.status = PipelineStatus.FAILED
        #     elif all([status == "COMPLETED" for status in process_exec_statuses]):
        #         print(f"Process execution COMPLETED for {self.id}")
        #         self.status = PipelineStatus.COMPLETED
        
    def load_metrics(self, pipeline_id: str, bids_layouts: Optional[list[BIDSLayout]] = None) -> None:
        # Load metrics from sidecar files
        if bids_layouts is None:
            bids_layouts = []
        for layout in bids_layouts:
            if self.metrics is None:
                self.metrics = []
            self.metrics.extend(get_sidecar_info_from_dataset(layout=layout, pipeline_id=pipeline_id))
            
    def save_metrics_to_db(self) -> None:
        if self.metrics is None:
            print(f"FAILED: Metrics for pipeline step {self.id} not calculated before saving to DB!")
            return
        for metric in self.metrics:
            filters: dict = ["process_id", "process_exec_id", "pipeline_id"]
            update_db_record(collection_name=COLLECTION_SUMMARIES, filters=filters, update=metric)

class Pipeline(BaseModel):
    id: str = Field(title="The ID of the pipeline.", default_factory=lambda: generate_id("PL", 6, "-"))
    name: str = Field(title="The name of the pipeline.")
    author: str = Field(title="Author.")
    description: str = Field(title="The description of the pipeline.")
    # version: str
    bids_roots: list[DirectoryPath] = Field(title="Paths to all datasets that the pipeline will run on. Sole purpose is to fetch results/metrics after execution.", default=[])
    steps: list[PipelineStep] = Field(title="List of pipeline steps.")
    checkpoint_steps: list[int] = Field(title="List of steps after which to pause the pipeline", default=[])
    
    @classmethod
    def from_user(cls, name: str, description: str, process_exec_ids: list[str], bids_roots: list[DirectoryPath] = [], checkpoint_steps: Optional[list[int]] = None) -> "Pipeline":
        if not checkpoint_steps:
            checkpoint_steps = []
        pipeline_steps: list[PipelineStep] = [PipelineStep.from_user(process_exec_id) for process_exec_id in process_exec_ids]
        return cls(
            name=name,
            description=description,
            bids_roots=bids_roots,
            steps=pipeline_steps,
            checkpoint_steps=checkpoint_steps
        )
    
    # def stop_all_processes(self):
    #     for step in self.steps:
    #         for process_exec in step.process_execs:
    #             print(f"Stopping process execution {process_exec.id}")
    #             process_exec.stop_container()
    
    def create_tree(self, tree: BIDSPipelineTree):
        """
        Create the pipeline tree.
        """
        for bids_root in self.bids_roots:
            derivatives_dir: PosixPath = bids_root / "derivatives" / self.name
            if derivatives_dir.exists():
                print(f"Directory {derivatives_dir} already exists. Skipping creation.")
                return
            
            derivatives_dir.mkdir(parents=True, exist_ok=True)
            with open(derivatives_dir / "dataset_description.json", "w") as f:
                f.write(tree.dataset_description.model_dump_json(indent=4))
            with open(derivatives_dir / "README", "w") as f:
                f.write(tree.readme_text)
            with open(derivatives_dir / "CITATION", "w") as f:
                f.write(tree.citation_text)
            with open(derivatives_dir / "CHANGES", "w") as f:
                f.write(tree.changes_text)
            with open(derivatives_dir / "LICENSE", "w") as f:
                f.write(tree.license_text)
    
    def execute(self, save_to_db: bool = True, retry_failed: bool = True):
        # ! Run this in Celery task
        
        tree: BIDSPipelineTree = BIDSPipelineTree()
        tree.set_default_values(name=self.name)
        tree.dataset_description = DatasetDescription(
            Name=self.name,
            DatasetType="derived",
            Authors=[self.author],
            # GeneratedBy=[GeneratedBy(
            #     Name=self.author, 
            #     Description="NeuroAnalyst", 
            #     Container={"Container": "Apptainer"},
            #     Version="0.0.1",
            #     CodeURL="https://github.com/chinmaymokashicm/neuroanalyst"
            #     )]
            GeneratedBy={
                "Name": self.author, 
                "Description": "NeuroAnalyst", 
                "Container": {"Container": "Apptainer"},
                "Version": "0.0.1",
                "CodeURL": "https://github.com/chinmaymokashicm/neuroanalyst"
            }
        )
        
        self.create_tree(tree)
        
        # Check connection first if save_to_db is set to True
        if save_to_db:
            check_connection()
        
        print(f"Executing pipeline {self.id}: {self.name}")
        bids_layouts: list[BIDSLayout] = []
        for bids_root in self.bids_roots:
            try:
                bids_layouts.append(BIDSLayout(bids_root, derivatives=True))
            except Exception as e:
                print(f"Could not load layout from {bids_root}: {e}")
                
        if save_to_db:
            self.to_db()
        for i, step in enumerate(self.steps, 1):
            if step.status == PipelineStatus.FAILED and not retry_failed:
                raise Exception(f"Pipeline step {step.id} in {self.id} failed.")
            if step.status == PipelineStatus.COMPLETED:
                print(f"Skipping step {i}: {step.name} as it has already completed.")
                continue
            
            print(f"Executing step {i}: {step.name}")
            step.execute(save_to_db=save_to_db, pipeline_id=self.id)
            
            # Stop pipeline if a step fails
            if step.status == PipelineStatus.FAILED:
                # self.update_step_status_in_db(i - 1, step.status)
                self.update_step_in_db(i - 1, step)
                step.load_metrics(pipeline_id=self.id, bids_layouts=bids_layouts)
                step.save_metrics_to_db()
                raise Exception(f"Pipeline step {step.id} in {self.id} failed.")
            
            # Update pipeline step in database
            if save_to_db:
                self.update_step_in_db(i - 1, step)
                step.load_metrics(pipeline_id=self.id, bids_layouts=bids_layouts)
                step.save_metrics_to_db()
            
            # Stop pipeline if a checkpoint is reached
            if i in self.checkpoint_steps:
                print(f"Checkpoint reached at step {i}. Pausing pipeline.")
                return
        
        print(f"Pipeline {self.id} completed successfully.")
        
        # except KeyboardInterrupt:
        #     print(f"Pipeline {self.id} interrupted.")
        #     # Kill all Docker containers
        #     self.stop_all_processes()
        
    def from_db(self, pipeline_id: str, connection: Optional[Connection] = None):
        pipeline_dict = find_one_from_db(COLLECTION_PIPELINES, {"id": pipeline_id}, connection)
        for idx, step in enumerate(pipeline_dict["steps"]):
            pipeline_dict["steps"][idx]["process_execs"] = []
            for process_exec_id in step["process_exec_ids"]:
                process_exec = ProcessExecApptainer.from_db(process_exec_id)
                pipeline_dict["steps"][idx]["process_execs"].append(process_exec)
        return Pipeline(**pipeline_dict)
    
    def to_db(self, connection: Optional[Connection] = None):
        steps: list[dict] = [{
            "id": step.id, 
            "name": step.name, 
            "process_exec_ids": [process_exec.id for process_exec in step.process_execs], 
            "status": step.status} for step in self.steps]
        pipeline: dict = {
            "id": self.id,
            "name": self.name,
            "description": self.description,
            "steps": steps,
            "checkpoint_steps": self.checkpoint_steps
        }
        insert_to_db(COLLECTION_PIPELINES, pipeline, connection)
    
    def update_step_in_db(self, step_id: str, step: PipelineStep, connection: Optional[Connection] = None):
        step_dict: dict = {"id": step.id, "name": step.name, "process_exec_ids": [process_exec.id for process_exec in step.process_execs], "status": step.status}
        update_db_record(COLLECTION_PIPELINES, {"id": self.id}, {f"steps.{step_id}": step_dict}, connection)
        
    def stop_pipeline(self):
        self.stop_all_processes()
        print(f"Pipeline {self.id} stopped.")