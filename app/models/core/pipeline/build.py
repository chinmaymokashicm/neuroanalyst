from ..process.execute import ProcessExecApptainer
from ..process.test import check_process_exec_status
from ....utils.db import Connection, insert_to_db, update_db_record, find_one_from_db, check_connection
from ....utils.constants import *
from ....utils.generate import generate_id
from ....utils.dataset import get_sidecar_info_from_dataset

from typing import Optional, Any
from pathlib import Path, PosixPath
from enum import Enum
import asyncio, time, json, logging
from asyncio import sleep

from imagelib.bids.pipeline import DatasetDescription, BIDSTree, GeneratedBy

from pydantic import BaseModel, Field, DirectoryPath
from bids import BIDSLayout
from bids.layout.models import BIDSJSONFile

logger = logging.getLogger(__name__)

class PipelineStatus(str, Enum):
    CREATED = "created"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

class PipelineStep(BaseModel):
    id: str = Field(title="The ID of the pipeline step.", default_factory=lambda: generate_id("PS", 6, "-"))
    name: str = Field(title="The name of the pipeline step.")
    process_execs: list[ProcessExecApptainer] = Field(title="List of process execution plans.")
    status: PipelineStatus = Field(title="The status of the pipeline step.", default=PipelineStatus.CREATED)
    metrics: Optional[list[dict]] = Field(title="Metrics generated by the pipeline step. Loaded from the dataset after execution.", default=None)
    
    @classmethod
    def from_user(cls, name: str, process_exec_ids: list[str]) -> "PipelineStep":
        process_execs: list[ProcessExecApptainer] = [ProcessExecApptainer.from_db(process_exec_id) for process_exec_id in process_exec_ids]
        return cls(
            name=name,
            process_execs=process_execs
        )
        
    def get_all_bids_roots(self) -> list[DirectoryPath]:
        """
        Get all BIDS roots from the process execution plans.
        """
        bids_roots: list[DirectoryPath] = []
        for process_exec in self.process_execs:
            if not hasattr(process_exec.process_exec_config, "output_volumes"):
                logger.warning(f"'output_volumes' not found in {process_exec.id}: {process_exec.process_exec_config}")
                continue
            if "data_dir" not in process_exec.process_exec_config.output_volumes:
                continue
            bids_root: DirectoryPath = process_exec.process_exec_config.output_volumes["data_dir"]
            if bids_root not in bids_roots:
                bids_roots.append(Path(bids_root))
        return bids_roots
    
    def create_all_bids_dirtrees(self, pipeline_name: str, authors: list[str] = [""]) -> None:
        tree: BIDSTree = BIDSTree()
        tree.set_default_values(name=self.name)
        tree.dataset_description = DatasetDescription(
            Name=pipeline_name,
            DatasetType="derived",
            Authors=authors,
            GeneratedBy=None
        )
        
        tree.dataset_description.GeneratedBy = [
            GeneratedBy(
                Name=pipeline_name, 
                Description="NeuroAnalyst", 
                Container={"Container": "Apptainer"},
                Version="0.0.1",
                CodeURL="https://github.com/chinmaymokashicm/neuroanalyst"
                )
        ]
        
        for bids_root in self.get_all_bids_roots():
            tree.create(bids_root / "derivatives" / pipeline_name)
    
    def execute(self, pipeline_id: str, pipeline_name: str, authors: list[str] = [""], save_to_db: bool = True):
        # ! Run this in Celery
        
        self.create_all_bids_dirtrees(pipeline_name=pipeline_name, authors=authors)
        
        self.status = PipelineStatus.RUNNING
        
        # Trigger run all processes in parallel
        for process_exec in self.process_execs:
            try:
                process_id, process_exec_id = process_exec.process_image.id, process_exec.id
                additional_envs: dict = {"PROCESS_ID": process_id, "PROCESS_EXEC_ID": process_exec_id, "PIPELINE_ID": pipeline_id, "PIPELINE_NAME": pipeline_name}
                process_exec.command = process_exec.generate_apptainer_run_command(additional_envs=additional_envs)
                update_db_record(COLLECTION_PROCESS_EXECS, {"id": process_exec.id}, {"command": process_exec.command})
                process_exec.execute(save_to_db=save_to_db)
            except Exception as e:
                self.status = PipelineStatus.FAILED
                raise Exception(f"Failed to execute process execution {process_exec.id}. Error: {e}")
        
        # Monitor for all processes to complete
        # while self.status == PipelineStatus.RUNNING:
        #     print(f"Checking status of process execution {process_exec.id} for {self.id}")
        #     time.sleep(5)
        #     process_exec_statuses = [check_process_exec_status(process_exec.id)["status"] for process_exec in self.process_execs]
        #     if "FAILED" in process_exec_statuses:
        #         print(f"Process execution FAILED for {self.id}")
        #         self.status = PipelineStatus.FAILED
        #     elif all([status == "COMPLETED" for status in process_exec_statuses]):
        #         print(f"Process execution COMPLETED for {self.id}")
        #         self.status = PipelineStatus.COMPLETED
        
    def load_metrics(self, pipeline_id: str, bids_layouts: Optional[list[BIDSLayout]] = None) -> None:
        # Load metrics from sidecar files
        if bids_layouts is None:
            bids_layouts = []
        for layout in bids_layouts:
            if self.metrics is None:
                self.metrics = []
            metrics: list = get_sidecar_info_from_dataset(layout=layout, pipeline_id=pipeline_id)
            if metrics is None:
                exception_message: str = "Metrics were not found. Something wrong?"
                logger.exception(exception_message)
                raise Exception(exception_message)
            self.metrics.extend(metrics)
            
    def save_metrics_to_db(self) -> None:
        if self.metrics is None:
            exception_message: str = f"FAILED: Metrics for pipeline step {self.id} not calculated before saving to DB!"
            logger.critical(exception_message)
            return
        for metric in self.metrics:
            insert_to_db(collection_name=COLLECTION_SUMMARIES, record=metric)

class Pipeline(BaseModel):
    id: str = Field(title="The ID of the pipeline.", default_factory=lambda: generate_id("PL", 6, "-"))
    name: str = Field(title="The name of the pipeline.")
    author: str = Field(title="Author.")
    description: str = Field(title="The description of the pipeline.")
    steps: list[PipelineStep] = Field(title="List of pipeline steps.")
    checkpoint_steps: list[int] = Field(title="List of steps after which to pause the pipeline", default=[])
    
    @classmethod
    def from_user(cls, name: str, author: str, description: str, process_exec_ids: list[str | list[str]], checkpoint_steps: Optional[list[int]] = None) -> "Pipeline":
        if not checkpoint_steps:
            checkpoint_steps = []
        # Every item in process_exec_ids represents one pipeline step. One pipeline step can have multiple process execs.
        step_wise_process_exec_ids: list[list[str]] = []
        for process_exec_id in process_exec_ids:
            if isinstance(process_exec_id, list):
                step_wise_process_exec_ids.append(process_exec_id)
            else:
                step_wise_process_exec_ids.append([process_exec_id])
        
        pipeline_steps: list[PipelineStep] = [PipelineStep.from_user(name=str(i), process_exec_ids=step_wise_process_exec_ids[i]) for i in range(len(step_wise_process_exec_ids))]
        return cls(
            name=name,
            author=author,
            description=description,
            steps=pipeline_steps,
            checkpoint_steps=checkpoint_steps
        )
    
    def get_all_bids_roots(self) -> list[DirectoryPath]:
        """
        Get all BIDS roots from the pipeline steps.
        """
        bids_roots: list[DirectoryPath] = []
        for step in self.steps:
            step_bids_roots: list[DirectoryPath] = step.get_all_bids_roots()
            for bids_root in step_bids_roots:
                if bids_root not in bids_roots:
                    bids_roots.append(bids_root)
        if len(bids_roots) == 0:
            logger.warning(f"No BIDS layouts found in {step.name}.")
        return bids_roots
    
    def execute(self, save_to_db: bool = True, retry_failed: bool = True):
        # ! Run this in Celery task
        
        # Check connection first if save_to_db is set to True
        if save_to_db:
            check_connection()
        
        logger.info(f"Executing pipeline {self.id}: {self.name}")
        bids_layouts: list[BIDSLayout] = []
        for bids_root in self.get_all_bids_roots():
            try:
                bids_layouts.append(BIDSLayout(bids_root, derivatives=True))
            except Exception as e:
                logger.critical(f"Could not load layout from {bids_root}: {e}")
                
        if len(bids_layouts) == 0:
            exception_message: str = "No BIDS layouts found. Looks like something went wrong."
            logger.exception(exception_message)
            logger.info(self.get_all_bids_roots())
            raise Exception(exception_message)

        for i, step in enumerate(self.steps, 1):
            if step.status == PipelineStatus.FAILED and not retry_failed:
                exception_message: str = f"Pipeline step {step.id} in {self.id} failed."
                logger.exception(exception_message)
                raise Exception(exception_message)
            if step.status == PipelineStatus.COMPLETED:
                logger.warning(f"Skipping step {i}: {step.name} as it has already completed.")
                continue
            
            logger.info(f"Executing step {i}: {step.name}", "\n\n\n\n")
            try:
                step.execute(
                    pipeline_id=self.id,
                    pipeline_name=self.name,
                    authors=[self.author], 
                    save_to_db=save_to_db
                )
            except Exception as e:
                exception_message: str = f"Pipeline step {step.id} in {self.id} failed."
                logger.exception(exception_message)
                if save_to_db:
                    self.update_step_in_db(i - 1, step)
                    step.load_metrics(pipeline_id=self.id, bids_layouts=bids_layouts)
                    step.save_metrics_to_db()
                raise Exception(exception_message)
            
            # Update pipeline step in database
            if save_to_db:
                logger.info(f"Saving step {i}:{step.name} metrics to DB.")
                self.update_step_in_db(i - 1, step)
                step.load_metrics(pipeline_id=self.id, bids_layouts=bids_layouts)
                step.save_metrics_to_db()
            
            # Stop pipeline if a checkpoint is reached
            if i in self.checkpoint_steps:
                logger.warning(f"Checkpoint reached at step {i}. Pausing pipeline.")
                return
        
        logger.info(f"Pipeline {self.id} completed successfully.")
        
    @classmethod
    def from_db(cls, pipeline_id: str, connection: Optional[Connection] = None):
        pipeline_dict = find_one_from_db(COLLECTION_PIPELINES, {"id": pipeline_id}, [], connection)
        id: str = pipeline_dict["id"]
        name: str = pipeline_dict["name"]
        author: str = pipeline_dict["author"]
        description: str = pipeline_dict["description"]
        steps: list[PipelineStep] = [PipelineStep.from_user(
            name=step["name"],
            process_exec_ids=step["process_exec_ids"]
        ) for step in pipeline_dict["steps"]]
            
        return cls(
            id=id,
            name=name,
            author=author,
            description=description,
            steps=steps
        )
    
    def to_db(self, connection: Optional[Connection] = None):
        steps: list[dict] = [{
            "id": step.id, 
            "name": step.name,
            "process_exec_ids": [process_exec.id for process_exec in step.process_execs], 
            "status": step.status} for step in self.steps]
        pipeline: dict = {
            "id": self.id,
            "name": self.name,
            "author": self.author,
            "description": self.description,
            "steps": steps,
            "checkpoint_steps": self.checkpoint_steps
        }
        insert_to_db(COLLECTION_PIPELINES, pipeline, connection)
    
    def update_step_in_db(self, step_id: str, step: PipelineStep, connection: Optional[Connection] = None):
        step_dict: dict = {"id": step.id, "name": step.name, "process_exec_ids": [process_exec.id for process_exec in step.process_execs], "status": step.status}
        update_db_record(COLLECTION_PIPELINES, {"id": self.id}, {f"steps.{step_id}": step_dict}, connection)
        
    def stop_pipeline(self):
        # self.stop_all_processes()
        # logger.info(f"Pipeline {self.id} stopped.")
        pass